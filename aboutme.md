---
layout: page
title: About me
subtitle: Until I can make my own Jarvis
---

## Introduce me
Hi. My name is Jiwon Park.

I've always been fascinated by how differently humans and AI approach problem-solving. While today's large language models keep getting bigger and more powerful, I'm skeptical that simply scaling up model size will lead us to AGI. What really intrigues me is how humans can solve complex problems with just a handful of examples and their prior knowledge – something that even advanced AI like ChatGPT still struggles with. Working under Prof. Sundong Kim's guidance, I'm exploring this gap by studying how the human brain learns and applying these insights to AI development. I'm particularly interested in the ARC Challenge community's innovative approaches to few-shot learning. My research aims to map human cognitive processes into latent spaces, potentially revolutionizing how AI systems learn. It's exciting to be at the crossroads of cognitive science and AI, working towards making machines learn more like humans do.

I have the following qualities:

- GIST(Gwangju Institute of Science and Technology) EECS(Department of Electrical Engineering and Computer Sciences) 20
- Best BA thesis award
- GIST AI graduate Ms & Ph.D. integrated 24 under prof.Sundong Kim.
  
## My achievement

"귀납 편향 제공을 위한 색채 어텐션 학습" - Jiwon Park, Hosung Lee, Jaehyun Park KSC2023 Thesis award (Domestic conference)
"Inductive bias provision for color attention learning" - Jiwon Park (Graduation Thesis)
"Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus" - Seungpil Lee, Woochang Sim, Donghyeon Shin, Wongyu Seo, Jiwon Park, Seokki Lee, Sanha Hwang, Sejin Kim, and Sundong Kim ACM Transactions on Intelligent Systems and Technology, 2025


## Favorite Paper
- Bober-Irizar, Mikel, and Soumya Banerjee. "Neural networks for abstraction and reasoning: Towards broad generalization in machines." arXiv preprint arXiv:2402.03507 (2024).
- Bonnet, Clément, and Matthew V. Macfarlane. "Searching Latent Program Spaces." arXiv preprint arXiv:2411.08706 (2024).
